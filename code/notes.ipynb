{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------Do not require-----------------\n",
    "# FAGANLAB = YKL                                                                            588 rows\n",
    "# UPEN_MASTER = ABETA, TAU, PTAU                                                            5877 rows\n",
    "# bateman = abeta40, abeta42, abeta42/abeta40                                               743 rows\n",
    "\n",
    "# -----------------NFL-----------------\n",
    "# Blennow = csf NFL                                                                            416 rows\n",
    "\n",
    "# -----------------ABETA42, ABETA40, ratio-----------------\n",
    "# FUJI = ABETA40, ABETA42 and abeta42/abeta40                                               443 rows\n",
    "# UPEN_ROSCHE = ABETA40 (few) , abeta42, tau, ptau                                          3175 rows\n",
    "\n",
    "# merge to check if yo ucan fill the -4 or empty values in either genetics or APOERS\n",
    "# APOERS = APVOLUME                                                                         2759 (1160) rows\n",
    "# GENETICS = APVOLUME                                                                       8000 rows\n",
    "\n",
    "# -----------------PTAU 181-----------------\n",
    "# ugotptau181 = ptau181                                                                     3759 rows\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------Notes----------------------------------------\n",
    "# Neurobiology of aging - 3.5 There was no difference between the carriers and non-carriers of the APOE ε4 allele in\n",
    "# any of the brain regions in neither the Aβ- or Aβ+ group\n",
    "\n",
    "\n",
    "# Do not take abeta40. Maybe take the abeta 42/40 ratio, with higher ratio associated with increased risk of alzhemier\n",
    "# Probably can check if it improves performance or something\n",
    "\n",
    "\n",
    "# Decresed levels of csf abeta42 \n",
    "# Decresed levels of ptau and tau\n",
    "\n",
    "# higher level of CSF t-tau, p-tau, neurogranin, neurofilament light associated with thinner cortex in the total group in regions including medial temporal cortex and superior cortex\n",
    "# May observe :\n",
    "# 1. higher t-tau associated with a thinner cortex mainly in lateral temporal\n",
    "# 2. higher p-tau showed associations with regions more specific to early AD, like entorhinal cortex (another paper could not find the relationship)\n",
    "\n",
    "# Right superior temporal areas were significantlt thinner in MCI patients after adjustment for age, gender and education\n",
    "\n",
    "# YKL-40 is not included, as from Jonathan's disseration, no infulence was found on the model's predictive accuracy and overall performance\n",
    "\n",
    "# Cortical thickeness of the medial temporal lobes was most severly reduced in AD patients. Even the lateral temporal lobes\n",
    "# Associations were found between MMSE scores and cortical thickness in temporal of the left hemisphere\n",
    "\n",
    "# Biomarkers of Cognitive Impairment: Abstract: In isolation, CSF levels show significant but weak associations with MMSE (Mini Mental State Exam)\n",
    "# Do not use MMSE\n",
    "\n",
    "# There is no mention of blood plasma neurofilament light. But higher levels of CSF neurofilament light is associated with\n",
    "# a thinner cortex in the total group in regions including the medial temporal cortex and the superior temporal cortex\n",
    "\n",
    "\n",
    "#  X means Cross sectional data and L means Longitudinal data\n",
    "\n",
    "\n",
    "# Might have to only include only baseline visits to avoid data independence issue, which is crutial for ml models. \n",
    "# This will also prevent covariance. But will lead to elimination pf half the dataset\n",
    "\n",
    "# Not taking upen_master: Do not need ABETA\n",
    "# Not using bateman for abeta40, abeta 42, and its ratio. Its a blood plasma data and not csf\n",
    "\n",
    "# May have to do the remove_repeated_id function where gerogio takes the mean of the repeated id if they have\n",
    "# same exam date and different values for that column\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------Extra-----------------------------\n",
    "# 1. Check the difference in version 6, 5.1, 4.4, 4.3 of cortical thickness excel. Maybe the unique RID that are not present in both the files??/\n",
    "# See what you are trying to do with the various target variables\n",
    "# Do we need to remove all the rows that have 0 for all the target variable?\n",
    "# We can merge cross-sectional and longitudinal, as we are considering longitudinal data individually, which becomes cross\n",
    "# how did jonathan get 340k count for the target varibles?\n",
    "# There is a 4,3 version Cross sectional, (But I guess its longitudinal). Could use that as well\n",
    "# ----------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------Tak=lking points-----------------\n",
    "# 1. Should I take longitudinal or cross sectional data?\n",
    "# Difference - \n",
    "# Longitudinal data has multiple rows for the same RID, with different exam dates, so it basically tracks the patient's biomarkers over time\n",
    "# Cross sectional data has only one row for each RID, so it basically gives the biomarkers of the patient at a particular time\n",
    "# But for some reason cross sectional has multiple row data for the same RID, with different exam dates. So it is basically longitudinal data\n",
    "\n",
    "# For now, do not take the 4.4 and 4.3, Geogiio had mentioned somewhere there was some change in the checmical used to measure the biomarkers\n",
    "# After 2014 or something\n",
    "\n",
    "# 2. There is 4.3 version and 4.4 version of the cortical thickness data. Should I take them as well?\n",
    "# If I do there are different values for the same RID in both the files. So I can take the mean of the repeated RIDs?\n",
    "\n",
    "\n",
    "\n",
    "# 1. check how many are availbale for all variables\n",
    "# count the data which has all the features. meaning count how many dependent variables are present for independent variables\n",
    "\n",
    "# 2. if we do not have much data then apply imputation methods such as KNN, MICE, etc\n",
    "# 3 If a lot of the dependent variables are missing, then do not use them\n",
    "\n",
    "# MMSE, as it shows weak associations with CSF levels in isolation\n",
    "# Maybe, it might improve performance when analysed with other features??\n",
    "\n",
    "\n",
    "# -----------------Questions-----------------\n",
    "# 1. Should I take NaN values as 0 or leave it as NaN? \n",
    "# If I keep it as NaN, then if there is one column with NaN, and the other with a non-NaN value, then I'll take the Non-NaN value, \n",
    "# but if I take it as 0, then I'll take its avg value\n",
    "# Which is the best approach?\n",
    "# Leave it as NaN\n",
    "\n",
    "# Probably will have to drop CSFNFL and ABETA42_40, as we have noc ount for them in the target variables\n",
    "\n",
    "# smote and adasym\n",
    "# 30 for testing\n",
    "# 44 for systehtic data\n",
    "\n",
    "# -> Cannot use SMOTE, as we have a multi regression problem, and SMOTE is used for classification problems\n",
    "# -> SMOTE is used for discrete values, not continuous values\n",
    "# -> The target variables will obviously be interdependent or will have some interdependecies between them,\n",
    "# so if I use SMOTE to generate synthetic data, using dependent variable and each individual target variable, the interdependencies will be ignored,\n",
    "# and it will generate unrealistic systhetic data that does not reflect the relationship between the target variables\n",
    "# -> ADASYN is the same as well, so cannot use that as well\n",
    "\n",
    "# -> Gaussian Process Regression (GPR) is a non-parametric, kernel-based approach to regression. \n",
    "# -> It asssues that the underlying data is drawn from a gaussian process, and can model complex non-linear relationships\n",
    "# -> Once you train a GPR model on your existing data, you can generate new synthetic data points by predicting values within the feature space where you want to create synthetic data\n",
    "# -> This will generate realistic synthetic data that reflects the underlying relationships between the features and the target variables\n",
    "\n",
    "# np.random.normal(0, y_std[:, None], y_pred.shape)\n",
    "# it takes mean and std as the first two arguments, get a normal distribution, selects a random number from the distribution and then, \n",
    "# after all the random variables are drawn, it produces an output in the provided shape as given in the third argument \n",
    "\n",
    "# -> There is Gaussian Copula as well. Maybe you could use that to generate synthetic data. Not sure\n",
    "\n",
    "# -> May have to hyperparamter tune the values in the kernel, such as the length scale, noise level, etc\n",
    "\n",
    "# -> Should I drop MH14ALCH, MH15DRUG, MH16SMOK, MH2NEURL, MHPSYCH??\n",
    "# -> What about APVOLUME, as 1256 of the 1292 values are -4 (meaning missing code)\n",
    "# Remove the -4 values and use the remaining data for the model\n",
    "\n",
    "# Start with AI architecture"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
