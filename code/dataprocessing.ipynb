{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genetics unique RIDs: 2719 len: 8302\n",
      "upenbiomk unique RIDs: 1660 len: 3174\n",
      "fugi_abeta unique RIDs: 423 len: 442\n",
      "ugotptau181 unique RIDs: 1191 len: 3758\n",
      "blenn_cfl_nfl unique RIDs: 399 len: 415\n",
      "ucsffsx6 unique RIDs: 1079 len: 2091\n",
      "ucsffsx51 unique RIDs: 689 len: 3311\n",
      "ptdemog unique RIDs: 4379 len: 5441\n",
      "medhist unique RIDs: 2491 len: 3083\n"
     ]
    }
   ],
   "source": [
    "# Load the required csv files\n",
    "registry_csv_file_path = \"../data/REGISTRY_29Oct2024.csv\"                       # For EXAMDATE and RID\n",
    "genetics_csv_file_path = \"../data/GENETIC_29Oct2024.csv\"                        # For APVOLUME\n",
    "upenbiomk_csv_file_path = \"../data/UPENNBIOMK_ROCHE_ELECSYS_29Oct2024.csv\"      # For ABETA42, TAU and PTAU\n",
    "fugi_abeta_path = \"../data/FUJIREBIOABETA_05Nov2024.csv\"                        # For ABETA42, ABETA42/40\n",
    "ugotptau181_csv_file_path = \"../data/UGOTPTAU181_06_18_20_29Oct2024.csv\"        # For PLASMAPTAU181\n",
    "blenn_cfl_nfl_path = \"../data/BLENNOWCSFNFL_05Nov2024.csv\"                      # For CSF Nfl\n",
    "ucsffsx6_csv_file_path = \"../data/UCSFFSX6_07_06_23_29Oct2024.csv\"              # For Cortical thickness in certain temporal region\n",
    "ucsffsx51_csv_file_path = \"../data/UCSFFSL51_03_01_22_29Oct2024.csv\"            # For Cortical thickness in ceratin temporal region\n",
    "ptdemog_csv_file_path = \"../data/PTDEMOG_29Oct2024.csv\"                         # For Patient gender and DOB\n",
    "medhist_csv_file_path = \"../data/MEDHIST_29Oct2024.csv\"                         # For medical history of patient\n",
    "\n",
    "\n",
    "# Selected columns\n",
    "registry_selected_columns = [\n",
    "    \"RID\",\n",
    "    \"EXAMDATE\"\n",
    "]\n",
    "\n",
    "genetics_selected_columns = [\n",
    "    \"RID\",\n",
    "    \"VISDATE\",\n",
    "    \"APVOLUME\"    \n",
    "]\n",
    "upenbiomk_selected_columns = [\n",
    "    \"RID\",\n",
    "    \"EXAMDATE\",\n",
    "    \"ABETA42\",\n",
    "    \"TAU\",\n",
    "    \"PTAU\"\n",
    "]\n",
    "fugi_abeta_columns = [\n",
    "    \"RID\",\n",
    "    \"EXAMDATE\", \n",
    "    \"ABETA42_40\",\n",
    "    \"ABETA42\",\n",
    "    # \"ABETA40\"\n",
    "]\n",
    "ugotptau181_selected_columns = [\n",
    "    \"RID\",\n",
    "    \"EXAMDATE\",\n",
    "    \"PLASMAPTAU181\"\n",
    "]\n",
    "blenn_cfl_nfl_columns = [\n",
    "    \"RID\",\n",
    "    \"EXAMDATE\", \n",
    "    \"CSFNFL\"\n",
    "]\n",
    "ucsffsx6_selected_columns = [\n",
    "    \"RID\",\n",
    "    \"EXAMDATE\",\n",
    "    \"ST58TA\",  # Cortical Thickness Average of Left Superior Temporal\n",
    "    \"ST117TA\", # Cortical Thickness Average of Right Superior Temporal\n",
    "    \"ST40TA\",  # Cortical Thickness Average of Left Middle Temporal\n",
    "    \"ST99TA\",  # Cortical Thickness Average of Right Middle Temporal\n",
    "    \"ST32TA\",  # Cortical Thickness Average of Left Inferior Temporal\n",
    "    \"ST91TA\",  # Cortical Thickness Average of Right Inferior Temporal\n",
    "    \"ST60TA\",  # Cortical Thickness Average of Left Temporal Pole\n",
    "    \"ST119TA\", # Cortical Thickness Average of Right Temporal Pole\n",
    "    \"ST62TA\",  # Cortical Thickness Average of Left Transverse Temporal\n",
    "    \"ST121TA\"  # Cortical Thickness Average of Right Transverse Temporal\n",
    "]\n",
    "ucsffsx51_selected_columns = [\n",
    "    \"RID\",\n",
    "    \"EXAMDATE\",\n",
    "    \"ST58TA\",  # Cortical Thickness Average of Left Superior Temporal\n",
    "    \"ST117TA\", # Cortical Thickness Average of Right Superior Temporal\n",
    "    \"ST40TA\",  # Cortical Thickness Average of Left Middle Temporal\n",
    "    \"ST99TA\",  # Cortical Thickness Average of Right Middle Temporal\n",
    "    \"ST32TA\",  # Cortical Thickness Average of Left Inferior Temporal\n",
    "    \"ST91TA\",  # Cortical Thickness Average of Right Inferior Temporal\n",
    "    \"ST60TA\",  # Cortical Thickness Average of Left Temporal Pole\n",
    "    \"ST119TA\", # Cortical Thickness Average of Right Temporal Pole\n",
    "    \"ST62TA\",  # Cortical Thickness Average of Left Transverse Temporal\n",
    "    \"ST121TA\"  # Cortical Thickness Average of Right Transverse Temporal\n",
    "]\n",
    "ptdemog_selected_columns = [\n",
    "    \"RID\",\n",
    "    \"VISDATE\",\n",
    "    \"PTGENDER\",\n",
    "    \"PTDOB\"\n",
    "]\n",
    "medhist_selected_columns = [\n",
    "    \"RID\",\n",
    "    \"VISDATE\",\n",
    "    \"MH14ALCH\",  # Alcohol Abuse\n",
    "    \"MH15DRUG\",  # Drug Abuse\n",
    "    \"MH16SMOK\",  # Smoking\n",
    "    \"MH2NEURL\",  # Neurologic (other than AD)\n",
    "    \"MHPSYCH\"   # Psychiatric Conditions\n",
    "]\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "registry_df = pd.read_csv(registry_csv_file_path, usecols=registry_selected_columns)\n",
    "genetics_df = pd.read_csv(genetics_csv_file_path, usecols=genetics_selected_columns)\n",
    "upenbiomk_df = pd.read_csv(upenbiomk_csv_file_path, usecols=upenbiomk_selected_columns)\n",
    "fugi_abeta_df = pd.read_csv(fugi_abeta_path, usecols= fugi_abeta_columns)\n",
    "ugotptau181_df = pd.read_csv(ugotptau181_csv_file_path, usecols=ugotptau181_selected_columns)\n",
    "blenn_cfl_nfl_df = pd.read_csv(blenn_cfl_nfl_path, usecols= blenn_cfl_nfl_columns)\n",
    "ucsffsx6_df = pd.read_csv(ucsffsx6_csv_file_path, usecols=ucsffsx6_selected_columns)\n",
    "ucsffsx51_df = pd.read_csv(ucsffsx51_csv_file_path, usecols=ucsffsx51_selected_columns)\n",
    "ptdemog_df = pd.read_csv(ptdemog_csv_file_path, usecols=ptdemog_selected_columns)\n",
    "medhist_df = pd.read_csv(medhist_csv_file_path, usecols=medhist_selected_columns)\n",
    "\n",
    "# dfs = [registry_df, genetics_df, upenbiomk_df, ugotptau181_df, ucsffsx51_df, ptdemog_df, medhist_df]\n",
    "dfs = [genetics_df, upenbiomk_df, fugi_abeta_df, blenn_cfl_nfl_df, ugotptau181_df, ucsffsx51_df, ptdemog_df, medhist_df]\n",
    "\n",
    "# print(f\"registry {registry_df['RID'].nunique()} len: {len(registry_df)}\")\n",
    "print(f\"genetics unique RIDs: {genetics_df['RID'].nunique()} len: {len(genetics_df)}\")\n",
    "print(f\"upenbiomk unique RIDs: {upenbiomk_df['RID'].nunique()} len: {len(upenbiomk_df)}\")\n",
    "print(f\"fugi_abeta unique RIDs: {fugi_abeta_df['RID'].nunique()} len: {len(fugi_abeta_df)}\")\n",
    "print(f\"ugotptau181 unique RIDs: {ugotptau181_df['RID'].nunique()} len: {len(ugotptau181_df)}\")\n",
    "print(f\"blenn_cfl_nfl unique RIDs: {blenn_cfl_nfl_df['RID'].nunique()} len: {len(blenn_cfl_nfl_df)}\")\n",
    "print(f\"ucsffsx6 unique RIDs: {ucsffsx6_df['RID'].nunique()} len: {len(ucsffsx6_df)}\")\n",
    "print(f\"ucsffsx51 unique RIDs: {ucsffsx51_df['RID'].nunique()} len: {len(ucsffsx51_df)}\")\n",
    "print(f\"ptdemog unique RIDs: {ptdemog_df['RID'].nunique()} len: {len(ptdemog_df)}\")\n",
    "print(f\"medhist unique RIDs: {medhist_df['RID'].nunique()} len: {len(medhist_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['RID', 'EXAMDATE', 'APVOLUME'], dtype='object')\n",
      "Index(['RID', 'EXAMDATE', 'PTGENDER', 'PTDOB'], dtype='object')\n",
      "Index(['RID', 'EXAMDATE', 'MHPSYCH', 'MH2NEURL', 'MH14ALCH', 'MH15DRUG',\n",
      "       'MH16SMOK'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Convert the column name from VISADATE TO EXAMDATE, as they mean the same\n",
    "for df in dfs:\n",
    "    if \"VISDATE\" in df.columns:\n",
    "        df.rename(columns={\"VISDATE\":\"EXAMDATE\"}, inplace=True)\n",
    "        print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  22 patients that have -4 APVOLUME value on the same exam date\n",
      "There are  0 patients that have -4 ABETA42 value on the same exam date\n",
      "There are  0 patients that have -4 TAU value on the same exam date\n",
      "There are  0 patients that have -4 PTAU value on the same exam date\n",
      "There are  0 patients that have -4 ABETA42 value on the same exam date\n",
      "There are  0 patients that have -4 ABETA42_40 value on the same exam date\n",
      "There are  0 patients that have -4 CSFNFL value on the same exam date\n",
      "There are  0 patients that have -4 PLASMAPTAU181 value on the same exam date\n",
      "There are  0 patients that have -4 ST32TA value on the same exam date\n",
      "There are  0 patients that have -4 ST40TA value on the same exam date\n",
      "There are  0 patients that have -4 ST58TA value on the same exam date\n",
      "There are  0 patients that have -4 ST60TA value on the same exam date\n",
      "There are  0 patients that have -4 ST62TA value on the same exam date\n",
      "There are  0 patients that have -4 ST91TA value on the same exam date\n",
      "There are  0 patients that have -4 ST99TA value on the same exam date\n",
      "There are  0 patients that have -4 ST117TA value on the same exam date\n",
      "There are  0 patients that have -4 ST119TA value on the same exam date\n",
      "There are  0 patients that have -4 ST121TA value on the same exam date\n",
      "There are  0 patients that have -4 PTGENDER value on the same exam date\n",
      "There are  0 patients that have -4 PTDOB value on the same exam date\n",
      "There are  0 patients that have -4 MHPSYCH value on the same exam date\n",
      "There are  0 patients that have -4 MH2NEURL value on the same exam date\n",
      "There are  0 patients that have -4 MH14ALCH value on the same exam date\n",
      "There are  0 patients that have -4 MH15DRUG value on the same exam date\n",
      "There are  0 patients that have -4 MH16SMOK value on the same exam date\n"
     ]
    }
   ],
   "source": [
    "for df in dfs:\n",
    "    for columns in df.columns:\n",
    "        if columns not in [\"RID\", \"EXAMDATE\"]:\n",
    "            duplicates = (\n",
    "                df.groupby([\"RID\", \"EXAMDATE\"])\n",
    "                .filter(lambda x: len(x)> 1 and x[columns].nunique()>1)\n",
    "            )\n",
    "            print(f\"There are  {len(duplicates[duplicates[columns] == -4])} patients that have -4 {columns} value on the same exam date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 22 patients that have -4 APVOLUME value on the same exam date\n"
     ]
    }
   ],
   "source": [
    "# There are data where for a patient, there are two values of APVOLUME on the same EXAMDATE. And one of them is -4\n",
    "# -4 is the Missing data code used by ADNI\n",
    "duplicates = (\n",
    "    genetics_df.groupby([\"RID\", \"EXAMDATE\"])\n",
    "    .filter(lambda x: len(x)> 1 and x[\"APVOLUME\"].nunique()>1)\n",
    ")\n",
    "print(\"There are\" , len(duplicates[duplicates[\"APVOLUME\"] == -4]), \"patients that have -4 APVOLUME value on the same exam date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genetics RID: 2719 len: 8280\n"
     ]
    }
   ],
   "source": [
    "# Dropping -4 values when they are present on the same day as the actual APVOLUME of the patient\n",
    "genetics_df = genetics_df.drop(duplicates[duplicates[\"APVOLUME\"] == -4].index)\n",
    "print(f\"genetics RID: {genetics_df['RID'].nunique()} len: {len(genetics_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upenbiomk_df\n",
      "    RID    EXAMDATE  ABETA42    TAU   PTAU\n",
      "38   42  2005-11-10   1258.0  250.1  18.23\n",
      "39   42  2006-11-09    766.5  205.4  18.40\n",
      "40   42  2008-08-18   1224.0  208.7  18.44\n",
      "41   42  2010-03-18   1011.0  211.5  18.10\n",
      "42   42  2011-04-14   1483.0  238.8  19.56\n",
      "43   42  2013-01-24   1139.0  218.4  17.69\n",
      "fugi_abeta_df\n",
      "   RID    EXAMDATE  ABETA42  ABETA42_40\n",
      "0   42  2011-04-14     1022       0.098\n"
     ]
    }
   ],
   "source": [
    "# There are 2 different values for ABETA42 for RID 42 Oon 2011-04-14\n",
    "# There are many more examples like these\n",
    "print(\"upenbiomk_df\")\n",
    "print(upenbiomk_df[upenbiomk_df[\"RID\"] == 42])\n",
    "print(\"fugi_abeta_df\")\n",
    "print(fugi_abeta_df[fugi_abeta_df[\"RID\"] == 42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge UPENN(Uni of Pennsylvania) and Fuji(Fujirebio) dataset\n",
    "merged_upen_fuji = pd.merge(upenbiomk_df, fugi_abeta_df, on=[\"RID\", \"EXAMDATE\"], suffixes=[\"_UPENN\", \"_FUJI\"], how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaNs with 0\n",
    "merged_upen_fuji[\"ABETA42_UPENN\"] = merged_upen_fuji[\"ABETA42_UPENN\"].fillna(0)     \n",
    "merged_upen_fuji[\"ABETA42_FUJI\"] = merged_upen_fuji[\"ABETA42_FUJI\"].fillna(0)\n",
    "\n",
    "# Calculate mean\n",
    "merged_upen_fuji[\"ABETA42\"] =  np.where(\n",
    "    # merged_upen_fuji[[\"ABETA42_UPENN\", \"ABETA42_FUJI\"]].notna().all(axis=1) &     # Uncomment if not doing the above fillna(0)\n",
    "    (merged_upen_fuji[[\"ABETA42_UPENN\", \"ABETA42_FUJI\"]] != 0).all(axis=1),         # Calculatye mean when values in both columns are non-zero\n",
    "    merged_upen_fuji[[\"ABETA42_UPENN\", \"ABETA42_FUJI\"]].mean(axis=1),              \n",
    "    np.where(                                                                       # If either of the value in the column is 0\n",
    "        # merged_upen_fuji[\"ABETA42_UPENN\"].notna() &                               # Uncomment if not doing the above fillna(0)\n",
    "        merged_upen_fuji[\"ABETA42_UPENN\"] != 0,                                     # Check if ABETA42_UPENN is non-zero\n",
    "        merged_upen_fuji[\"ABETA42_UPENN\"],                                          # Use ABETA42_UPENN if valid\n",
    "        merged_upen_fuji[\"ABETA42_FUJI\"]                                            # Else ABETA42_FUJI\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the ABETA42_UPENN and ABETA42_FUJI column\n",
    "merged_upen_fuji.drop(columns=[\"ABETA42_UPENN\", \"ABETA42_FUJI\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RID</th>\n",
       "      <th>EXAMDATE</th>\n",
       "      <th>TAU</th>\n",
       "      <th>PTAU</th>\n",
       "      <th>ABETA42_40</th>\n",
       "      <th>ABETA42</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>42</td>\n",
       "      <td>2005-11-10</td>\n",
       "      <td>250.1</td>\n",
       "      <td>18.23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1258.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>42</td>\n",
       "      <td>2006-11-09</td>\n",
       "      <td>205.4</td>\n",
       "      <td>18.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>766.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>2008-08-18</td>\n",
       "      <td>208.7</td>\n",
       "      <td>18.44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>2010-03-18</td>\n",
       "      <td>211.5</td>\n",
       "      <td>18.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1011.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>2011-04-14</td>\n",
       "      <td>238.8</td>\n",
       "      <td>19.56</td>\n",
       "      <td>0.098</td>\n",
       "      <td>1252.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>42</td>\n",
       "      <td>2013-01-24</td>\n",
       "      <td>218.4</td>\n",
       "      <td>17.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1139.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    RID    EXAMDATE    TAU   PTAU  ABETA42_40  ABETA42\n",
       "38   42  2005-11-10  250.1  18.23         NaN   1258.0\n",
       "39   42  2006-11-09  205.4  18.40         NaN    766.5\n",
       "40   42  2008-08-18  208.7  18.44         NaN   1224.0\n",
       "41   42  2010-03-18  211.5  18.10         NaN   1011.0\n",
       "42   42  2011-04-14  238.8  19.56       0.098   1252.5\n",
       "43   42  2013-01-24  218.4  17.69         NaN   1139.0"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_upen_fuji[merged_upen_fuji[\"RID\"] == 42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the UPENN(Uni of Pennsylvania) and Fuji(Fujirebio) dataset from the list of datasets\n",
    "dfs = [df for df in dfs if not df.equals(upenbiomk_df) and not df.equals(fugi_abeta_df)]\n",
    "\n",
    "# Add the merged dataset of UPENN(Uni of Pennsylvania) and Fuji(Fujirebio) to the list of datasets\n",
    "dfs.append(merged_upen_fuji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_rows = []                                                                                               # Remove it, its only there to check\n",
    "filtered_rows_to_check = []\n",
    "\n",
    "def filter_patients_within_six_months(group):\n",
    "    # Ensure EXAMDATE is in datetime format\n",
    "    group[\"EXAMDATE\"] = pd.to_datetime(group[\"EXAMDATE\"] , errors='coerce')\n",
    "\n",
    "    # Sort by EXAMDATE\n",
    "    group.sort_values(by=\"EXAMDATE\", ascending=True, inplace=True)\n",
    "\n",
    "    # List containing the rows that are atleast 6 months apart for each patient\n",
    "    filtered_rows = []\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        # Check if 'filtered_rows' is empty or if the difference in EXAMDATE is atleast 6 months\n",
    "        if not filtered_rows or row[\"EXAMDATE\"] >= filtered_rows[-1][\"EXAMDATE\"] + pd.DateOffset(months=6):\n",
    "            filtered_rows.append(row)\n",
    "            filtered_rows_to_check.append(row)\n",
    "        else:                                                                                                   # Remove it, its only there to check\n",
    "            removed_rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(filtered_rows)\n",
    "\n",
    "for df in dfs:\n",
    "    df = df.groupby(\"RID\", group_keys=False).apply(filter_patients_within_six_months).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RID                         125\n",
      "EXAMDATE    2011-05-26 00:00:00\n",
      "APVOLUME                   -4.0\n",
      "Name: 2702, dtype: object, RID                              125\n",
      "EXAMDATE         2011-05-26 00:00:00\n",
      "PLASMAPTAU181                 18.265\n",
      "Name: 113, dtype: object]\n"
     ]
    }
   ],
   "source": [
    "# Print patient with \"RID\"=125 that was removed because \n",
    "# the patient had an EXAMDATE within 6 months\n",
    "print([s for s in removed_rows if s.get(\"RID\") ==125])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RID                         125\n",
       " EXAMDATE    2006-01-05 00:00:00\n",
       " APVOLUME                    6.0\n",
       " Name: 95, dtype: object,\n",
       " RID                         125\n",
       " EXAMDATE    2011-05-26 00:00:00\n",
       " APVOLUME                    8.0\n",
       " Name: 1580, dtype: object,\n",
       " RID                         125\n",
       " EXAMDATE    2012-06-27 00:00:00\n",
       " APVOLUME                   -4.0\n",
       " Name: 2606, dtype: object,\n",
       " RID                         125\n",
       " EXAMDATE    2013-06-05 00:00:00\n",
       " APVOLUME                   -4.0\n",
       " Name: 3846, dtype: object,\n",
       " RID                         125\n",
       " EXAMDATE    2015-01-22 00:00:00\n",
       " APVOLUME                   -4.0\n",
       " Name: 5268, dtype: object,\n",
       " RID                              125\n",
       " EXAMDATE         2010-03-02 00:00:00\n",
       " PLASMAPTAU181                 76.067\n",
       " Name: 111, dtype: object,\n",
       " RID                              125\n",
       " EXAMDATE         2011-05-26 00:00:00\n",
       " PLASMAPTAU181                  25.84\n",
       " Name: 112, dtype: object,\n",
       " RID                              125\n",
       " EXAMDATE         2012-06-27 00:00:00\n",
       " PLASMAPTAU181                 24.128\n",
       " Name: 114, dtype: object,\n",
       " RID                              125\n",
       " EXAMDATE         2013-06-05 00:00:00\n",
       " PLASMAPTAU181                  9.652\n",
       " Name: 115, dtype: object,\n",
       " RID                         125\n",
       " EXAMDATE    2006-01-05 00:00:00\n",
       " PTGENDER                    1.0\n",
       " PTDOB                   04/1932\n",
       " Name: 99, dtype: object,\n",
       " RID                         125\n",
       " EXAMDATE    2011-05-26 00:00:00\n",
       " PTGENDER                    1.0\n",
       " PTDOB                   04/1932\n",
       " Name: 2186, dtype: object,\n",
       " RID                         125\n",
       " EXAMDATE    2006-01-05 00:00:00\n",
       " MHPSYCH                       0\n",
       " MH2NEURL                      0\n",
       " MH14ALCH                      0\n",
       " MH15DRUG                      0\n",
       " MH16SMOK                      0\n",
       " Name: 92, dtype: object,\n",
       " RID                         125\n",
       " EXAMDATE    2011-05-26 00:00:00\n",
       " MHPSYCH                       0\n",
       " MH2NEURL                      1\n",
       " MH14ALCH                      0\n",
       " MH15DRUG                      0\n",
       " MH16SMOK                      0\n",
       " Name: 1703, dtype: object]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See for the exact date of 2011-05-26\n",
    "in_filtered = [s for s in filtered_rows_to_check if s.get(\"RID\") == 125]\n",
    "in_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16878"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "merged_df = reduce(lambda left, right: pd.merge(left, right, on=[\"RID\", \"EXAMDATE\"], how='outer'), dfs)\n",
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RID</th>\n",
       "      <th>APVOLUME</th>\n",
       "      <th>CSFNFL</th>\n",
       "      <th>PLASMAPTAU181</th>\n",
       "      <th>ST32TA</th>\n",
       "      <th>ST40TA</th>\n",
       "      <th>ST58TA</th>\n",
       "      <th>ST60TA</th>\n",
       "      <th>ST62TA</th>\n",
       "      <th>ST91TA</th>\n",
       "      <th>ST99TA</th>\n",
       "      <th>ST117TA</th>\n",
       "      <th>ST119TA</th>\n",
       "      <th>ST121TA</th>\n",
       "      <th>PTGENDER</th>\n",
       "      <th>MHPSYCH</th>\n",
       "      <th>MH2NEURL</th>\n",
       "      <th>MH14ALCH</th>\n",
       "      <th>MH15DRUG</th>\n",
       "      <th>MH16SMOK</th>\n",
       "      <th>TAU</th>\n",
       "      <th>PTAU</th>\n",
       "      <th>ABETA42_40</th>\n",
       "      <th>ABETA42</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16878.000000</td>\n",
       "      <td>6790.000000</td>\n",
       "      <td>415.000000</td>\n",
       "      <td>3819.000000</td>\n",
       "      <td>3330.000000</td>\n",
       "      <td>3330.000000</td>\n",
       "      <td>3330.000000</td>\n",
       "      <td>3330.000000</td>\n",
       "      <td>3330.000000</td>\n",
       "      <td>3330.000000</td>\n",
       "      <td>3330.000000</td>\n",
       "      <td>3330.000000</td>\n",
       "      <td>3330.000000</td>\n",
       "      <td>3330.000000</td>\n",
       "      <td>5344.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3179.000000</td>\n",
       "      <td>3167.000000</td>\n",
       "      <td>446.000000</td>\n",
       "      <td>3215.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4951.293874</td>\n",
       "      <td>1.340884</td>\n",
       "      <td>1465.816867</td>\n",
       "      <td>18.961121</td>\n",
       "      <td>2.717821</td>\n",
       "      <td>2.725335</td>\n",
       "      <td>2.574538</td>\n",
       "      <td>3.469113</td>\n",
       "      <td>2.210322</td>\n",
       "      <td>2.765076</td>\n",
       "      <td>2.780147</td>\n",
       "      <td>2.598437</td>\n",
       "      <td>3.505418</td>\n",
       "      <td>2.246651</td>\n",
       "      <td>1.426085</td>\n",
       "      <td>0.348801</td>\n",
       "      <td>0.314394</td>\n",
       "      <td>0.043876</td>\n",
       "      <td>0.009470</td>\n",
       "      <td>0.398043</td>\n",
       "      <td>286.838191</td>\n",
       "      <td>27.283227</td>\n",
       "      <td>0.065126</td>\n",
       "      <td>1034.281913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>34353.643034</td>\n",
       "      <td>6.294932</td>\n",
       "      <td>1020.097069</td>\n",
       "      <td>14.719812</td>\n",
       "      <td>0.239194</td>\n",
       "      <td>0.229197</td>\n",
       "      <td>0.226311</td>\n",
       "      <td>0.455701</td>\n",
       "      <td>0.248271</td>\n",
       "      <td>0.236469</td>\n",
       "      <td>0.213069</td>\n",
       "      <td>0.209134</td>\n",
       "      <td>0.517448</td>\n",
       "      <td>0.263617</td>\n",
       "      <td>0.751417</td>\n",
       "      <td>0.476666</td>\n",
       "      <td>0.464347</td>\n",
       "      <td>0.204852</td>\n",
       "      <td>0.096866</td>\n",
       "      <td>0.489572</td>\n",
       "      <td>128.473103</td>\n",
       "      <td>14.278210</td>\n",
       "      <td>0.044928</td>\n",
       "      <td>607.768732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>0.362000</td>\n",
       "      <td>1.613000</td>\n",
       "      <td>1.579000</td>\n",
       "      <td>1.575000</td>\n",
       "      <td>1.341000</td>\n",
       "      <td>1.363000</td>\n",
       "      <td>1.575000</td>\n",
       "      <td>1.626000</td>\n",
       "      <td>1.528000</td>\n",
       "      <td>1.238000</td>\n",
       "      <td>1.308000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>80.080000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1338.250000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>967.500000</td>\n",
       "      <td>11.105000</td>\n",
       "      <td>2.581000</td>\n",
       "      <td>2.615000</td>\n",
       "      <td>2.442000</td>\n",
       "      <td>3.243000</td>\n",
       "      <td>2.047000</td>\n",
       "      <td>2.639000</td>\n",
       "      <td>2.678000</td>\n",
       "      <td>2.472000</td>\n",
       "      <td>3.282000</td>\n",
       "      <td>2.075000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>195.650000</td>\n",
       "      <td>17.165000</td>\n",
       "      <td>0.041250</td>\n",
       "      <td>579.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4339.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1250.000000</td>\n",
       "      <td>16.343000</td>\n",
       "      <td>2.743500</td>\n",
       "      <td>2.754000</td>\n",
       "      <td>2.592000</td>\n",
       "      <td>3.541500</td>\n",
       "      <td>2.219000</td>\n",
       "      <td>2.794000</td>\n",
       "      <td>2.799000</td>\n",
       "      <td>2.615000</td>\n",
       "      <td>3.615000</td>\n",
       "      <td>2.251000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>257.000000</td>\n",
       "      <td>23.540000</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>845.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5066.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1644.000000</td>\n",
       "      <td>23.516500</td>\n",
       "      <td>2.877000</td>\n",
       "      <td>2.878000</td>\n",
       "      <td>2.730750</td>\n",
       "      <td>3.775750</td>\n",
       "      <td>2.381000</td>\n",
       "      <td>2.927000</td>\n",
       "      <td>2.918000</td>\n",
       "      <td>2.736000</td>\n",
       "      <td>3.855000</td>\n",
       "      <td>2.427000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>344.700000</td>\n",
       "      <td>33.485000</td>\n",
       "      <td>0.089750</td>\n",
       "      <td>1389.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>999999.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>12647.000000</td>\n",
       "      <td>451.398000</td>\n",
       "      <td>3.424000</td>\n",
       "      <td>3.294000</td>\n",
       "      <td>3.151000</td>\n",
       "      <td>4.684000</td>\n",
       "      <td>2.960000</td>\n",
       "      <td>3.381000</td>\n",
       "      <td>3.409000</td>\n",
       "      <td>3.270000</td>\n",
       "      <td>4.548000</td>\n",
       "      <td>3.006000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1018.000000</td>\n",
       "      <td>108.500000</td>\n",
       "      <td>0.827000</td>\n",
       "      <td>4779.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 RID     APVOLUME        CSFNFL  PLASMAPTAU181       ST32TA  \\\n",
       "count   16878.000000  6790.000000    415.000000    3819.000000  3330.000000   \n",
       "mean     4951.293874     1.340884   1465.816867      18.961121     2.717821   \n",
       "std     34353.643034     6.294932   1020.097069      14.719812     0.239194   \n",
       "min         1.000000    -4.000000    405.000000       0.362000     1.613000   \n",
       "25%      1338.250000    -4.000000    967.500000      11.105000     2.581000   \n",
       "50%      4339.000000    -4.000000   1250.000000      16.343000     2.743500   \n",
       "75%      5066.000000     9.000000   1644.000000      23.516500     2.877000   \n",
       "max    999999.000000    10.000000  12647.000000     451.398000     3.424000   \n",
       "\n",
       "            ST40TA       ST58TA       ST60TA       ST62TA       ST91TA  \\\n",
       "count  3330.000000  3330.000000  3330.000000  3330.000000  3330.000000   \n",
       "mean      2.725335     2.574538     3.469113     2.210322     2.765076   \n",
       "std       0.229197     0.226311     0.455701     0.248271     0.236469   \n",
       "min       1.579000     1.575000     1.341000     1.363000     1.575000   \n",
       "25%       2.615000     2.442000     3.243000     2.047000     2.639000   \n",
       "50%       2.754000     2.592000     3.541500     2.219000     2.794000   \n",
       "75%       2.878000     2.730750     3.775750     2.381000     2.927000   \n",
       "max       3.294000     3.151000     4.684000     2.960000     3.381000   \n",
       "\n",
       "            ST99TA      ST117TA      ST119TA      ST121TA     PTGENDER  \\\n",
       "count  3330.000000  3330.000000  3330.000000  3330.000000  5344.000000   \n",
       "mean      2.780147     2.598437     3.505418     2.246651     1.426085   \n",
       "std       0.213069     0.209134     0.517448     0.263617     0.751417   \n",
       "min       1.626000     1.528000     1.238000     1.308000    -4.000000   \n",
       "25%       2.678000     2.472000     3.282000     2.075000     1.000000   \n",
       "50%       2.799000     2.615000     3.615000     2.251000     1.000000   \n",
       "75%       2.918000     2.736000     3.855000     2.427000     2.000000   \n",
       "max       3.409000     3.270000     4.548000     3.006000     2.000000   \n",
       "\n",
       "           MHPSYCH     MH2NEURL     MH14ALCH     MH15DRUG     MH16SMOK  \\\n",
       "count  3168.000000  3168.000000  3168.000000  3168.000000  3168.000000   \n",
       "mean      0.348801     0.314394     0.043876     0.009470     0.398043   \n",
       "std       0.476666     0.464347     0.204852     0.096866     0.489572   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     1.000000     0.000000     0.000000     1.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "               TAU         PTAU  ABETA42_40      ABETA42  \n",
       "count  3179.000000  3167.000000  446.000000  3215.000000  \n",
       "mean    286.838191    27.283227    0.065126  1034.281913  \n",
       "std     128.473103    14.278210    0.044928   607.768732  \n",
       "min      80.080000     8.000000    0.022000     0.000000  \n",
       "25%     195.650000    17.165000    0.041250   579.325000  \n",
       "50%     257.000000    23.540000    0.055000   845.150000  \n",
       "75%     344.700000    33.485000    0.089750  1389.000000  \n",
       "max    1018.000000   108.500000    0.827000  4779.000000  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4382"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[\"RID\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RID    EXAMDATE  MMSE\n",
      "0    2  2005-09-08  28.0\n",
      "1    3  2005-09-12  20.0\n",
      "2    3  2006-03-13  24.0\n",
      "3    3  2006-09-12  17.0\n",
      "4    3  2007-09-12  19.0\n"
     ]
    }
   ],
   "source": [
    "# Ask if to include MMSE, as it shows weak associations with CSF levels in isolation\n",
    "# Maybe, it might improve performance when analysed with other features??\n",
    "adnimerge_path = \"../data/ADNIMERGE_05Nov2024.csv\"\n",
    "adnimerge_columns = [\n",
    "    \"RID\", \n",
    "    \"EXAMDATE\", \n",
    "    \"MMSE\"\n",
    "]\n",
    "\n",
    "adnimerge_df = pd.read_csv(adnimerge_path, usecols=adnimerge_columns)\n",
    "print(adnimerge_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
